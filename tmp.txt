This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesThis paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.Article history: Received 12 August 2019 Received in revised form 2 December 2019 Accepted 20 December 2019 Available online 2 January 2020 Keywords: Multi-Objective Evolutionary Algorithms  Graphic Processing Unit  High Performance Computing  Gridding Stochastic non-domination sorting This work introduces an accelerated implementation of NSGA-2 on a graphics processing unit  to reduce execution time.They are designed to be efficiently vectorized on the GPU, therefore, the proposed approach is finally limited by the sorting procedure (O (nlog)), while the original algorithm was of order O.This improvement is reflected on the speedups attained in the experiments.© 2020 Elsevier B. V. All rights reservedMulti-objective evolutionary algorithms  have established themselves as a solid alternative to traditional methods in the last years.Classic approaches like weighted sum, lexicographical methods or goal programming should be applied many times to obtain different solutions [1], while MOEAs have the ability to find a close approximation to the Pareto front in a single run [2].In the literature, different methods have been proposed to solve this issue.Some authors have seen the potential of combining GPU accelerated code with other nature-inspired methods.For example, Arnaldo et al.[27] proposed a GPU genetic programming algorithm with the ability of managing large datasets.In comparison with the related works, this implementation has a further level of parallelism than other references because all the components of NSGA-2 are run into the GPU, while the other works has focused on parallelization of some modules of it, specially NDS.The original NSGA-2 has the following components: NDS, crowding, and genetic operators.In this section, the vectorized versions of them are explained.In the case of NDS and crowding, the original algorithms need to be modified to be efficiently vectorized.The concept of Pareto dominance is essential to understand NDS.Pertinent definitions are included for the sake of completeness.We can define an MOP in the following manner [22]: min y =f  = (f1, f2,.,The original NDS uses domination to measure the contribution of each individual towards the construction of a "good" Pareto front.Next Generation Discarded Individuals Fig.The intuition behind this modification is the following: Non-dominated individuals will have 0 ranks regardless the number of individuals they are compared to them, while the individuals from the last non-dominated front have a high probability to be dominated by a small sample of individuals.In NSGA-2, crowding is the density metric used to ensure diversity of the population [2].The grid is imposed over the search space.It can be dynamically updated applying max  e min  Where on is the number of desired cells along each grid's dimension.In grid-crowding, the metric is the normalized number of neighbors an individual has inside its corresponding cell.This is accomplished using 1 ℮ sgn sgn) × 1 0 1.We can find some references in regards to full-vectorized, parallel genetic operators [41].This mask is created from Mc = m, /m, where mine is a matrix of random numbers of size n x m. Thus, the mask is defined as follows: Finally, we can use this mask to combine the population g with the shuffled population goes goes = g ®  $ gas & Mk.Mutation We can implement mutation in a similar manner than the one explained above.In this case, the mask is mm = significant (-sign (Imc e Rand| ℮ pm) € 1)  Where Pm is the mutation probability.Selection is performed in a similar way than the original NSGA-2.We start with a population of N individuals and genetic operators are applied to create N children.Stochastic NDS and grid-crowding are computed for all of them, being the latter step the difference with the original procedure.We can see a flowchart of the proposed approach in Fig.These problems have different properties in regards of number of variables, convexity, connectivity  solutions, etc..The performance metrics included in this study are the ones used by Deb et al.in their study [2].Are In the experiments, the effects of algorithm configuration were explored using different parameters.86264365198935E−05 Algorithm Performance  -- - - NSGA2mod GNSGA2 Performance 600 800 1000 Population size data of 100 runs were collected to compute the metrics for each different combination of population size and total generations.They show the averaged results of NSGA-2 for the performance metrics and different problems.Time speedups are observed for all the benchmarks when comparing GNSGA-2 to NSGA-2.Algorithm Performance  --- NSGA2mod GNSGA2 Performance, Performance Population size Fig.Although, the shape of the figures mentioned above seems to indicate the complexity of both versions is dominated by the sorting procedure, as expected.This paper has introduced an implementation of NSGA-2 where time speedup is the main concern.k has a direct impact on computation time, given the operators have a complexity of O.This means k could be a fixed number independent of n. Lower values of k speedup the algorithm, but induce uncertainty in the selection process.In regards of the other references, the present approach proposed a higher level of parallelism in the sense all the components of the algorithm are run on the device while other works had focused on specific parts of the algorithm.No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have an impending conflict with this work.Anton Aguilar-Rivera: Conceptualization, Methodology, Software, Validation, InvestigationThe author thanks to Consejo Nacional de Ciencia y Tecnología, Mexico for the postdoctoral fellowship grant at Barcelona Supercomputing Center[1] R. T. Marler, J. S. Arora, Survey  multi-objective optimization methods for engineering, Struct.14] X.Sun, L. Zhao, P. Zhang, L. Bao, Y. Chen, Enhanced NSGA-II with an evolving directions prediction for interval multi-objective optimization, Swarm Evol.28] K. Wang, Z. Shen, A GPU-based parallel genetic algorithm for generating daily activity plans, IEEE Trans.29] P. Krömer, J. Platoš, V. Snášel, A. Abraham, A comparison of many-threaded differential evolution and genetic algorithms on CUDA, in: 2011 Third World Congress on Nature and Biologically Inspired Computing, IEEE, 2011, pp.38] C. Garcia, G. Botella, F. Ayuso, M. Prieto, F. Tirado, Multi-GPU based on multicriteria optimization of the motion estimation system, EURASIP J. Adv.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.Our goal is not to declare an overall winner.In fact, we find that for some workloads ULE is better and for others CFS is better.We first examine the impact of the per-core scheduling decisions made by ULE and CFS, by running applications and combinations of applications on a single core, We then run the applications on all cores on the machine, and study the impact of the load balancer.The outline of the rest of this paper is as follows.Section 3 describes our port of ULE to Linux and the main differences between the native ULE implementation and our port.Per-core scheduling: Linux's CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].Since CFS schedules the thread with the lowest vruntime, CFS needs to prevent a thread from having a brainstorm much lower than the vruntimes of the other threads waiting to be scheduled.Using the minimum vruntime also ensures that threads that sleep a lot are scheduled first, a desirable strategy on desktop systems, because it minimizes the latency of interactive applications.Load balancing also happens periodically.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.The Linux kernel offers an API to add new schedulers to the kernel.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.Because of that, we had to slightly change the ULE load balancing to avoid migrating a currently running thread.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.Interactivity penalty Interactivity penalty Interactivity penalty of five Interactivity penalty of sysbench threads 20 40 60 80 100 120 140 160 180 Time  Figure 2: Interactivity penalty of threads over time.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.Overall, the scheduler has little influence on most workloads.The apache workload consists of two applications: the main server  running 100 threads, and ab, a single threaded load injector.In this section, we analyze the impact of the load balancing and thread placement strategies on performance.Next, we compare the performance of 37 applications running on CFS vs.. ULE.Finally, we analyze the performance of multiplication workloadsCFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Thread counts below 15 are represented in shades of gray.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.The delay is more than 50% because threads scheduled alone on their cores go to sleep, and then have to be woken up, thus adding latency to the barriers.This suboptimal thread placement also explains the performance difference between CFS and ULE on FT and UA.Contrary to Figure 6, threads do not start pinning on core 020 C-Ray DCraw johnjohn- johnscimark2- scimark2scimark2- scimark2scimark2- scimark2Build-apache 10 1 12 13 14 15 16 17 18 12 20 21 2 23 24 15 16 27 28 29 30 1 12 3 34 35 6 37 8 39 0 4 2 23 4 5 45 47 48 9 50 5 22 blackscholes fluidanimate streamcluster Figure 8: Performance of ULE with respect to CFS on a multicore.To validate this assumption, we replaced the ULE wakeup function by a simple one that returns the CPU on which the thread was previously running, and then observed no difference between ULE and CFS.6% is the highest time spent in the scheduler we observed in CFS.Finally, we evaluate the combination of interactive and background workloads using a set of different applications: c-ray + EP  is a workload where both applications are considered background by ULE, fibo + sysbench and blackscholes + ferret are workloads where only one application is interactive, and apache + sysbench is a fully interactive workload.This is expected, as they schedule background threads in a similar way.Surprisingly, when co-scheduled with fibo, sysbench performs worse on ULE than on CFS even though it is correctly categorized as interactive and gets priority over five threads.Previous works have compared the design choices made by FreeBSD and Linux.CFS, multiple ULE single app ULE multiple c-ray Batch + interactive Interactive + interactive blackscholes apache sysbench Figure 9: Performance of CFS and ULE on multi application workloads with respect to the performance of the application running alone on CFS.average waiting time of threads in scheduler run queues.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.Scheduling threads on a multicore machine is hard.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 2005 ACM conference on Emerging network experiment and technology, ACM, pp.23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.Our goal is not to declare an overall winner.In fact, we find that for some workloads ULE is better and for others CFS is better.We first examine the impact of the per-core scheduling decisions made by ULE and CFS, by running applications and combinations of applications on a single core, We then run the applications on all cores on the machine, and study the impact of the load balancer.The outline of the rest of this paper is as follows.Section 3 describes our port of ULE to Linux and the main differences between the native ULE implementation and our port.Per-core scheduling: Linux's CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].Since CFS schedules the thread with the lowest vruntime, CFS needs to prevent a thread from having a brainstorm much lower than the vruntimes of the other threads waiting to be scheduled.Using the minimum vruntime also ensures that threads that sleep a lot are scheduled first, a desirable strategy on desktop systems, because it minimizes the latency of interactive applications.Load balancing also happens periodically.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.The Linux kernel offers an API to add new schedulers to the kernel.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.Because of that, we had to slightly change the ULE load balancing to avoid migrating a currently running thread.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.Interactivity penalty Interactivity penalty Interactivity penalty of five Interactivity penalty of sysbench threads 20 40 60 80 100 120 140 160 180 Time  Figure 2: Interactivity penalty of threads over time.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.Overall, the scheduler has little influence on most workloads.The apache workload consists of two applications: the main server  running 100 threads, and ab, a single threaded load injector.In this section, we analyze the impact of the load balancing and thread placement strategies on performance.Next, we compare the performance of 37 applications running on CFS vs.. ULE.Finally, we analyze the performance of multiplication workloadsCFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Thread counts below 15 are represented in shades of gray.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.The delay is more than 50% because threads scheduled alone on their cores go to sleep, and then have to be woken up, thus adding latency to the barriers.This suboptimal thread placement also explains the performance difference between CFS and ULE on FT and UA.Contrary to Figure 6, threads do not start pinning on core 020 C-Ray DCraw johnjohn- johnscimark2- scimark2scimark2- scimark2scimark2- scimark2Build-apache 10 1 12 13 14 15 16 17 18 12 20 21 2 23 24 15 16 27 28 29 30 1 12 3 34 35 6 37 8 39 0 4 2 23 4 5 45 47 48 9 50 5 22 blackscholes fluidanimate streamcluster Figure 8: Performance of ULE with respect to CFS on a multicore.To validate this assumption, we replaced the ULE wakeup function by a simple one that returns the CPU on which the thread was previously running, and then observed no difference between ULE and CFS.6% is the highest time spent in the scheduler we observed in CFS.Finally, we evaluate the combination of interactive and background workloads using a set of different applications: c-ray + EP  is a workload where both applications are considered background by ULE, fibo + sysbench and blackscholes + ferret are workloads where only one application is interactive, and apache + sysbench is a fully interactive workload.This is expected, as they schedule background threads in a similar way.Surprisingly, when co-scheduled with fibo, sysbench performs worse on ULE than on CFS even though it is correctly categorized as interactive and gets priority over five threads.Previous works have compared the design choices made by FreeBSD and Linux.CFS, multiple ULE single app ULE multiple c-ray Batch + interactive Interactive + interactive blackscholes apache sysbench Figure 9: Performance of CFS and ULE on multi application workloads with respect to the performance of the application running alone on CFS.average waiting time of threads in scheduler run queues.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.Scheduling threads on a multicore machine is hard.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 2005 ACM conference on Emerging network experiment and technology, ACM, pp.23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.Our goal is not to declare an overall winner.In fact, we find that for some workloads ULE is better and for others CFS is better.We first examine the impact of the per-core scheduling decisions made by ULE and CFS, by running applications and combinations of applications on a single core, We then run the applications on all cores on the machine, and study the impact of the load balancer.The outline of the rest of this paper is as follows.Section 3 describes our port of ULE to Linux and the main differences between the native ULE implementation and our port.Per-core scheduling: Linux's CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].Since CFS schedules the thread with the lowest vruntime, CFS needs to prevent a thread from having a brainstorm much lower than the vruntimes of the other threads waiting to be scheduled.Using the minimum vruntime also ensures that threads that sleep a lot are scheduled first, a desirable strategy on desktop systems, because it minimizes the latency of interactive applications.Load balancing also happens periodically.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.The Linux kernel offers an API to add new schedulers to the kernel.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.Because of that, we had to slightly change the ULE load balancing to avoid migrating a currently running thread.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.Interactivity penalty Interactivity penalty Interactivity penalty of five Interactivity penalty of sysbench threads 20 40 60 80 100 120 140 160 180 Time  Figure 2: Interactivity penalty of threads over time.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.Overall, the scheduler has little influence on most workloads.The apache workload consists of two applications: the main server  running 100 threads, and ab, a single threaded load injector.In this section, we analyze the impact of the load balancing and thread placement strategies on performance.Next, we compare the performance of 37 applications running on CFS vs.. ULE.Finally, we analyze the performance of multiplication workloadsCFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Thread counts below 15 are represented in shades of gray.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.The delay is more than 50% because threads scheduled alone on their cores go to sleep, and then have to be woken up, thus adding latency to the barriers.This suboptimal thread placement also explains the performance difference between CFS and ULE on FT and UA.Contrary to Figure 6, threads do not start pinning on core 020 C-Ray DCraw johnjohn- johnscimark2- scimark2scimark2- scimark2scimark2- scimark2Build-apache 10 1 12 13 14 15 16 17 18 12 20 21 2 23 24 15 16 27 28 29 30 1 12 3 34 35 6 37 8 39 0 4 2 23 4 5 45 47 48 9 50 5 22 blackscholes fluidanimate streamcluster Figure 8: Performance of ULE with respect to CFS on a multicore.To validate this assumption, we replaced the ULE wakeup function by a simple one that returns the CPU on which the thread was previously running, and then observed no difference between ULE and CFS.6% is the highest time spent in the scheduler we observed in CFS.Finally, we evaluate the combination of interactive and background workloads using a set of different applications: c-ray + EP  is a workload where both applications are considered background by ULE, fibo + sysbench and blackscholes + ferret are workloads where only one application is interactive, and apache + sysbench is a fully interactive workload.This is expected, as they schedule background threads in a similar way.Surprisingly, when co-scheduled with fibo, sysbench performs worse on ULE than on CFS even though it is correctly categorized as interactive and gets priority over five threads.Previous works have compared the design choices made by FreeBSD and Linux.CFS, multiple ULE single app ULE multiple c-ray Batch + interactive Interactive + interactive blackscholes apache sysbench Figure 9: Performance of CFS and ULE on multi application workloads with respect to the performance of the application running alone on CFS.average waiting time of threads in scheduler run queues.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.Scheduling threads on a multicore machine is hard.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 2005 ACM conference on Emerging network experiment and technology, ACM, pp.23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.Our goal is not to declare an overall winner.In fact, we find that for some workloads ULE is better and for others CFS is better.We first examine the impact of the per-core scheduling decisions made by ULE and CFS, by running applications and combinations of applications on a single core, We then run the applications on all cores on the machine, and study the impact of the load balancer.The outline of the rest of this paper is as follows.Section 3 describes our port of ULE to Linux and the main differences between the native ULE implementation and our port.Per-core scheduling: Linux's CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].Since CFS schedules the thread with the lowest vruntime, CFS needs to prevent a thread from having a brainstorm much lower than the vruntimes of the other threads waiting to be scheduled.Using the minimum vruntime also ensures that threads that sleep a lot are scheduled first, a desirable strategy on desktop systems, because it minimizes the latency of interactive applications.Load balancing also happens periodically.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.The Linux kernel offers an API to add new schedulers to the kernel.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.Because of that, we had to slightly change the ULE load balancing to avoid migrating a currently running thread.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.Interactivity penalty Interactivity penalty Interactivity penalty of five Interactivity penalty of sysbench threads 20 40 60 80 100 120 140 160 180 Time  Figure 2: Interactivity penalty of threads over time.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.Overall, the scheduler has little influence on most workloads.The apache workload consists of two applications: the main server  running 100 threads, and ab, a single threaded load injector.In this section, we analyze the impact of the load balancing and thread placement strategies on performance.Next, we compare the performance of 37 applications running on CFS vs.. ULE.Finally, we analyze the performance of multiplication workloadsCFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Thread counts below 15 are represented in shades of gray.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.The delay is more than 50% because threads scheduled alone on their cores go to sleep, and then have to be woken up, thus adding latency to the barriers.This suboptimal thread placement also explains the performance difference between CFS and ULE on FT and UA.Contrary to Figure 6, threads do not start pinning on core 020 C-Ray DCraw johnjohn- johnscimark2- scimark2scimark2- scimark2scimark2- scimark2Build-apache 10 1 12 13 14 15 16 17 18 12 20 21 2 23 24 15 16 27 28 29 30 1 12 3 34 35 6 37 8 39 0 4 2 23 4 5 45 47 48 9 50 5 22 blackscholes fluidanimate streamcluster Figure 8: Performance of ULE with respect to CFS on a multicore.To validate this assumption, we replaced the ULE wakeup function by a simple one that returns the CPU on which the thread was previously running, and then observed no difference between ULE and CFS.6% is the highest time spent in the scheduler we observed in CFS.Finally, we evaluate the combination of interactive and background workloads using a set of different applications: c-ray + EP  is a workload where both applications are considered background by ULE, fibo + sysbench and blackscholes + ferret are workloads where only one application is interactive, and apache + sysbench is a fully interactive workload.This is expected, as they schedule background threads in a similar way.Surprisingly, when co-scheduled with fibo, sysbench performs worse on ULE than on CFS even though it is correctly categorized as interactive and gets priority over five threads.Previous works have compared the design choices made by FreeBSD and Linux.CFS, multiple ULE single app ULE multiple c-ray Batch + interactive Interactive + interactive blackscholes apache sysbench Figure 9: Performance of CFS and ULE on multi application workloads with respect to the performance of the application running alone on CFS.average waiting time of threads in scheduler run queues.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.Scheduling threads on a multicore machine is hard.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 2005 ACM conference on Emerging network experiment and technology, ACM, pp.23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.