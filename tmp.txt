This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesThis paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.