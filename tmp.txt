This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesThis paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.I N recent years, interest has grown rapidly towards harnessing the power of graphics hardware to perform general-purpose parallel computing—so-called general purpose graphics processing units  computing.N. Goswami and Tao Li are with the Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611.Although the latest version  of the publicly available GPGPU-Sim enables asynchronous kernel launches from the CPU using thread parallelism, the GPU simulation engine itself is single-threaded [8]. )acceleration techniques for CPUs and GPUs are not suitable for GPGPU.GPGPU refers to running General-Purpose computation on Graphics Processing Units.1, a typical GPU consists of a number of streaming multi-processors, each containing multiple streaming processor  cores.Compute unified device architecture is a programming model developed for NVIDIA GPUs, which allows programmers to write programs using C functions called kernels [2], [11].consist of one or more kernels, which can run concurrently from the Fermi architecture [12] onwards.Before proposing our solution to the GPGPU architecture simulation challenge, we first revisit existing CPU simulation acceleration techniques.We do this by first characterizing existing CUDA kernels to provide supportive quantitative evidence during the analysis and discussion of this study.The number of static basic blocks in a CUDA kernel ranges from 10 to 25 for most benchmarks, with an average of 22.5 illustrates the number of dynamically executed instructions per thread.There exist a number of CPU simulation acceleration techniques, which we revisit now in the context of GPGPU: sampled simulation in time and space, statistical simulation, and reduced input sets.As the threads of a kernel may share hardware resources, including global memory or interconnection network, sampling in space is likely to change several of the important GPGPU performance characteristics such as branch divergence [19] and memory coalescing behavior.Input data size  NW  # of basic blocks average: 22.500 percentage branch instructions average: 8.proportional to the execution of the basic block in the original workload.A significant concern with reduced input sets for GPGPU workloads is that performance  heavily correlates with problem size, as illustrated in Fig.to graphics applications: an average number of 4.We now propose GPGPU workload synthesis to address the GPGPU simulation challenge.In the final step, the synthetic benchmark is simulated on an execution-driven architectural simulator, such as GPGPU-Sim.Authorized licensed use limited to: Chung-on Univ.thread execute times ------- n ------ >> percentage branch instructions  ocl BoxFilter ocl MarchingCubes oclMedianFilter oclParticles ocl SimpleGL oclSobelFilter ocl VolumeRender ocl PostprocessGL oclRecursiveGaussian oclSimple Texture3D Fig.During kernel profiling, we collect a number of program characteristics that collectively capture the inherent execution characteristics of GPGPU workloads.As mentioned in Section 2, the batch of threads that execute a kernel is organized as a grid of CTAs.For GPGPU, different instructions may have dramatically different throughput [27], [28].0, the throughput of the native 32-bit floating-point add instruction is 32, while it is only 4 for the 32-bit floating-point instructions reciprocal and reciprocal square root [27].Capturing the control flow behavior for each thread by collecting a trace for each thread would be prohibitively costly because of the massive number of threads in a GPGPU kernel.Count is the loop count and quantifies how many times the given basic block was iterated over in a loop by all threads.A dash box near the start of a solid arrow denotes the statistics of a basic block’s out-edge.Shared memory is as fast as a register on a typical GPU.In order to preserve similar shared memory bank conflict behavior in the synthetic clone, we need to clone the shared arrays and their access behavior from the original to the synthesized version.We therefore profile the shared arrays which includes collecting  the number of shared arrays,  the data type and size of each array, and  the way the arrays are accessed.GPUs provide a memory coalescing mechanism to improve memory performance by merging global memory accesses from different threads within a warp into a single memory transaction if contiguous memory addresses are accessed [2].We account for both cases, and capture an array’s base address and its index.As for shared array accesses, we reproduce the same memory access patterns as long as the address is a fine expression of the thread IDHaving described how we collect a profile of a GPGPU kernel in the previous section, we now detail on how we generate a synthetic cloneOur synthetic benchmark generation framework aims at faithfully mimicking control flow, branch divergence and memory access behavior.We therefore describe loop and control flow behavior in more detail now in the current and the next section.11 illustrates these three typical loop patterns.Note that different threads executing a CUDA loop may iterate the loop for a different number of times, which leads to branch divergence behavior.This situation is quite common in CUDA benchmarks based on our observationTo achieve similar performance between the synthetic clone and the original GPGPU kernel, it is crucial to preserve its divergence and control flow behavior.For the ease of reasoning, we partition the control flow graph of a kernel into two parts, namely the outside and the inside-loop area as shown in Fig.In the discussion to follow, we define the global thread ID as graded ¼ blockNInGrid.with threadNInBlock the index of the given thread in its thread block or CTA, blockNInGrid the index of the given block within the grid, and threadPerBlock the number of threads per block.while the remaining threads go the other way.A number of threads go in one direction, the rest goes in the other direction, and the sum is greater than the number of incoming threads to the jump point.>> Count    1  2 51200 51200 3 1 6400 6400.The existence of loops in CUDA threads provides us with an opportunity to reduce simulation time while preserving performance and behavior.The basic intuition is that reducing the loop iteration count reduces simulation time of a kernel while preserving execution behavior in terms of the number of instructions executed per cycle.One key question now is how to determine the reduction factor.We now describe the various steps to generate a synthetic miniature kernel.14 shows the overview of our code Authorized licensed use limited to: Chung-on Univ.11, NOVEMBER 2015 Create a kernel object Creating basic blocks Set divergence rate global __ void kernel; -- 0->1; 0->3; 1->2; 2->2; 2->3; 3->4; 3->5; 4->5; 5->6; O --- ECount: 17536 O --- SCount: 17536 O --- LCount: 0 Out Edeges: -- OECount: 17536 *** TCount:17536 On Edeges: --- OECount:0 *** TCount:0 On Edeges: --- OECount:0 *** TCount:0 main _function  allocate host and device memory I kernel <<< gridDim, blockDim>>>; global void kernel  calculating global trade; create opcode for basic blocks global trade Param_space: 7 SETP_OP: %p 1.We follow a top-down hierarchy: kernel, basic block, and instruction.There are also auxiliary codes generated between the different levels in the hierarchy to set information such as kernel parameters and divergence rate to control the execution of the synthesized kernel.10) Generate code to calculate the global thread ID using the Formula.Authorized licensed use limited to: Chung-on Univ.As previously mentioned, we analyzed instruction, basic block, and thread characteristics for 35 benchmarks from the CUDA SDK [36], Parboil [38], and Rodinia [39] benchmark suites.Fifteen # of threads Insts/thread MumerGPU  Petri-Net Simulation LU Decomposition Matrix Transpose Breadth First Search Needleman-Wunsch Scan Nearest Neighbor Histogram 64 Black-Scholes Cellular Automation 3D Laplace Solver LIBOR Monte Carlo 3D stencil computation Magnetic Resonance Imaging FHD Scalar Product Particle Filter Matrix multiplication Neural Network Leukocyte Tracking Levenshtein Distance benchmarks were randomly selected out of the top-25 long-running benchmarks, see Fig.The other eight benchmarks  are those that could not be accelerated well through sampling in space, according to Fig.In this section, we first evaluate the efficacy of GPGPUMiniBench.We consider the following factors in the evaluation of GPGPU-MiniBench:  the impact of the reduction factor in simulation speed and accuracy,  achieved simulation speedup,  accuracy, and  other metrics such as shared memory bank conflicts, memory coalescing and branch divergence behavior.We define simulation speedup as follows: speedup ¼ simulation time original simulation time synthetic - with simulation time original the time to simulate the original benchmark on the GPGPU performance simulator, and - IEEE TRANSACTIONS ON COMPUTERS, VOL.As previously mentioned, GPGPU-MiniBench reduces simulation time by decreasing loop iteration counts using a reduction factor.rounded down value of the result is 1, then the maximum reduction factor is obtained.GPGPU-MINIBENCH: ACCELERATING GPGPU MICRO-ARCHITECTURE SIMULATION Luff, relative error DEL A ÑODODANONA 0000 IPC Error PF MM NN LT LV STO AVG Fig.For example, the performance difference seems to be small between the four GPU architectures for the CL, PF and NN benchmarks.Since GPGPU-MiniBench reduces simulation time by reducing the number of loop iterations in the synthesized code to one, one may think that setting the number of loop iterations to one in CUDA source code directly might be equally accurate while being much simpler to implement.Authorized licensed use limited to: Chung-on Univ.As can be seen, loop reduction significantly changes the static control flow graph of MM, while GPGPU-MiniBench does notMicro-architecture simulation is a key tool for computer architecture research and development.Only recently has interest grown regarding GPGPU architecture simulation, as exemplified by GPGPU-Sim [5], [8], Ocelot [6], [19] and Barra [7].Although TBPoint achieves high accuracy while simulating 10 to 20 percent of the total execution time of the kernel, sampling workloads with high control/memory divergence behavior remains challenging.Slow micro-architecture simulation speed has been a major and constant concern for several decades in the CPU, domain, and now with the emergence of GPGPU computing, there is a strong need for simulation acceleration techniques for GPGPU.We therefore proposed a very different approach in this paper.× GPGPU-MiniBench is accurate enough for making high-level design decisions and trend analyses in GPGPU architectures and systemsThis work was supported by the National Basic Research Program of China  under grant No.61272132, the CAS special program for the next generation of information technology for sensing China under grant No.XDA06010500, and in part by NSF under grant CCF-1016966.[1] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kruger, A. E. Lefohn, and T. J. Purcell, “A survey of general-purpose computation on graphics hardware,” in Proc.23] A. Joshi, L. Eeckhout, R. H. Bell Jr, and L. K. John, “Performance cloning: A technique for disseminating proprietary applications as benchmarks,” in Proc.She received the US National Science Foundation  CAREER Award in 1996, UT Austin Engineering Foundation Faculty Award in 2001, Halliburton, Brown and Root Engineering Foundation Young Faculty Award in 1999, University of Texas Alumni Association Teaching Award in 2004, The Pennsylvania State University Outstanding Engineering Alumnus in 2011 etc.. She holds eight U. S. patents and has published 16 book chapters, and approximately 200 papers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.Section 7 presents related work and Section 8 concludesPer-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.A negative nice value  makes it easier for a thread to be considered interactive.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.The 16 Phoronix applications are: compilation benchmarks, compression, image processing, scientific, cryptography  and web.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.This paper analyzes the impact on application performance of the design and implementation choices made in two widely used open-source schedulers: ULE, the default FreeBSD scheduler, and CFS, the default Linux scheduler.We compare ULE and CFS in otherwise identical circumstances.ULE may cause starvation, even when executing a single application with identical threads, but this starvation may actually lead to better application performance for some workloads.Operating system kernel schedulers is responsible for maintaining high utilization of hardware resources  while providing faster response time to latency-sensitive applications.To isolate the effect of differences between CFS and ULE, we have ported ULE to Linux, and we use it as the default scheduler to run all threads on the machine.The outline of the rest of this paper is as follows.Per-core scheduling: Linux’s CFS implements a weighted fair queueing algorithm: it evenly divides CPU cycles between threads weighted by their priority  [18].CFS also uses heuristics to improve cache usage.Load balancing: In a multi-user setting, Linux’s CFS evens out the amount of work on all cores of the machine.CFS takes into account the loads of cores when creating or waking up threads.For instance, if CFS detects a 1-to-many producer-consumer pattern, then it spreads out the consumer threads as much as possible on the machine, and most cores on the machine are considered suitable to host woken up threads.Per-core scheduling: ULE uses two runqueues to schedule threads: one run queue contains interactive threads, and the other contains batch threads.A third runqueue called idle is used when a core is idle.The goal of having two runqueues is to give priority to interactive threads.This metric is defined as a function of the timer a thread has spent running and the times a thread has spent voluntarily sleeping, and is computed as follows: scaling factor = m = 50 penalties =.s A penalty in the lower half of the range  means that a thread has spent more time voluntarily sleeping than running, while a penalty above means the opposite.To classify threads, ULE first computes a score defined as interactivity penalty + niceness.With a nice value of 0, this corresponds roughly to spending more than 60% of the time sleeping.When a thread is created, it inherits the runtime and sleep time  of its parent.Inside the interactive runqueue, there is one FIFO per priority.To avoid starvation between batch threads, ULE tries to be fair among batch threads by minimizing the difference of runtime between threads, similar to what CFS does with the vruntime.However, the value of a timeslice change with the number of threads currently running on the core.If this also fails, ULE simply picks the car with the lowest number of running threads on the machine.ULE also balances threads when the interactive and batch runqueues of a core are empty.In this section we describe the problems encountered when porting ULE to Linux, and the main differences between our port and the original FreeBSD code.In the few cases where the interfaces do not match, it was possible to find a workaround.In ULE, a thread that runs on a core is removed from the runqueue data structure, and added back when its time slice is depleted, so that the FIFO property holds.We evaluate ULE on a 32-core machine  with 32GB of RAM.To assess the performance of CFS and ULE, we used both synthetic benchmarks and realistic applications.Fib is a synthetic application computing Fibonacci numbers.Hackbench [10] is a benchmark designed by the Linux community to stress the scheduler.In this section, we run applications on a single core to avoid possible side effects introduced by the load balancer.We first analyze the impact of this design decision by co-scheduling a batch and an interactive application on the same core, and we show that under ULE batch applications can starve for an unbounded amount of time.We then show that starvation under ULE can occur even when the system is only running a single application.In this section, we analyze the behavior of CFS and ULE running a multi-application workload consisting of a compute-intensive thread that never sleeps, and an application whose threads mostly sleep.Thus, sysbench threads get absolute priority over the fiber thread.Sysbench runs 50% slower on CFS, because it shares the CPU with fibo, instead of running in isolation, as it does with ULE.The starvation exhibited by ULE in the multi-application scenario above also occurs in single-application workloads.We now exemplify this behavior using sysbench.In sysbench, the master thread is created with the interactivity penalty in the bash process from which it was forked.Since bash mostly sleeps, sysbench is created as an interactive process.Since these threads spend most of their time waiting for new requests, their interactivity penalty stays low, and their priority remains higher than that  threads that were forked late in the initialization process.The threads created to later never execute.As a consequence, ULE has a lower average latency than CFS.100 Interactivity penalty of five Interactivity penalty of sysbench threads 80 100 120 140 160 180 Figure 2: Interactivity penalty of threads over time.Normalized Runtime, Runtime of the master thread Runtime of interactive threads Runtime of background threads 60 Time  Figure 3: Cumulative runtime of threads of sysbench on ULE.Some are created with a low penalty, and their penalty decreases as they execute, while other threads are created with a high penalty and never execute.We now analyze the impact of the per-core scheduling on the performance of 37 applications.Figure 5 presents the performance difference between CFS and ULE on a single core, with percentages above 0 meaning that the application executes faster with ULE than CFS.When the application is executed with ULE, the compute thread can be delayed, because Java system threads are considered interactive and get priority over the computation thread.CFS relies on a rather complex load metric.All threads perform the same work, so we expect the load balancer to place 16 threads on each of the 32 cores.Each line represents a core, time passes on the x-axis, and colors represent the numbers of threads on the core.We study the placement of threads using c-ray, an image processing application from the Phoronix benchmark suite.Then all threads wait at a barrier before performing the computation.Since all threads behave in the same way, we would expect ULE to perform better than CFS  that configuration: ULE always forks threaded on the core with the lowest number of threads, so the load should be perfectly balanced from the start.Figure 8 presents the performance difference between CFS and ULE in a multicore context.Figure 9 shows the only one application is interactive, and apache + syssysbench and blackscholes + ferret are workloads where applications are considered background by ULE, fibo + tons: c-ray + EP  is a workload where both background workloads using a set of different applicaFinally, we evaluate the combination of interactive and Multi application workloads overhead of ULE is 1%.Groves et al.[9] compare the overhead of CFS against BFS, a simplistic scheduler aimed at improving the responsiveness on machines with few cores.[1] [PATCH] Fix bug in which the long term ULE load balancer is executed only once.In Proceedings of the 5th Workshop on Operating Systems Platforms for Embedded Real-Time Applications, OSPERT.[ 23] WONG, C. , TAN, I. , KUMARI, R. , LAM, J. , AND FUN, W. Fairness and interactive performance of O and CFS Linux kernel schedulers.